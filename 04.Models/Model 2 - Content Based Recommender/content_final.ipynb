{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Zd8x2srTV0GB","7For-tkzPhio","uB43R8KSiBpf","JN8zOXd9ibDC"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"Zd8x2srTV0GB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"op0r2zgfUXKW","executionInfo":{"status":"ok","timestamp":1688036952298,"user_tz":-120,"elapsed":511,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43f95ad6-c461-488e-91db-85177ef635ae"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import json\n","from scipy.sparse import load_npz\n","from sklearn.metrics.pairwise import cosine_similarity\n","import scipy.sparse as sp\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import ast\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["# Function to select multiple user IDs & timestamps for evaluation\n","def select_user_ids_timestamps(k=5):\n","  # Select the top 10 rows\n","  filtered_behaviors_df = behaviors_df.tail(k)\n","\n","  # Create a list of tuples containing values from columns 'a' and 'b'\n","  user_ids_timestamps = [(row['User ID'], row['Timestamp']) for _, row in filtered_behaviors_df.iterrows()]\n","\n","  return user_ids_timestamps"],"metadata":{"id":"71CkfvXXNm2y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Data"],"metadata":{"id":"7For-tkzPhio"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# import behaviors df\n","behaviors_df = pd.read_csv(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/behaviors.csv\")\n","\n","# import news df\n","news_df = pd.read_pickle(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/news.pkl\")\n","#news_df['Release Date'] = pd.to_datetime(news_df['Release Date'])\n"],"metadata":{"id":"vVNefdNo0Wfd","executionInfo":{"status":"ok","timestamp":1688036893879,"user_tz":-120,"elapsed":26840,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7a1f0a7-d33d-46e2-d27d-7ff6f64ede6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Content Based Model"],"metadata":{"id":"uB43R8KSiBpf"}},{"cell_type":"markdown","source":["## Define Functions for Content Based Filtering\n","\n"],"metadata":{"id":"jaeaEujvWNCn"}},{"cell_type":"markdown","source":["### Embeddings"],"metadata":{"id":"QzcrwW7RYypA"}},{"cell_type":"code","source":["def single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=5):\n","   # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_news_vector = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), 'Average Vector'].mean()\n","\n","  # Filter news_df to exlcude articles in user history\n","  filtered_news_df = news_df.loc[~news_df['News ID'].isin(previously_read_article_ids)]\n","\n","  # Convert input timestamp to date time\n","  timestamp = pd.to_datetime(timestamp)\n","\n","  # # Filter news_df to exlcude any articles released after date of interaction\n","  # filtered_news_df = filtered_news_df[filtered_news_df['Release Date'] <= timestamp]\n","\n","  # Compute cosine similarity between average_news_vector and each unread news article\n","  filtered_news_df['Similarity'] = filtered_news_df['Average Vector'].apply(lambda x: cosine_similarity([average_news_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  filtered_news_df = filtered_news_df.sort_values(by='Similarity', ascending=False)\n","\n","  #select top k articles\n","  final_recommended_ids = filtered_news_df.head(articles_k)['News ID'].tolist()\n","\n","  return final_recommended_ids\n"],"metadata":{"id":"sPpuEm2hZ81n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content_embeddings(user_ids_timestamps, articles_k=5):\n","  '''Inputs:\n","  user_ids_timestaps: tuple with user_id & timestamp\n","  '''\n","  # create an empty dictionary to populate with recommendations\n","  user_recommendations_dict = {}\n","\n","  for user_id, timestamp in user_ids_timestamps:\n","    # run function for single users\n","    final_recommended_ids = single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=articles_k)\n","\n","    # create dictionary for final recommendations\n","    user_recommendations_dict[user_id] = final_recommended_ids\n","\n","  return user_recommendations_dict"],"metadata":{"id":"xSjesU0gc5a-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TFIDF"],"metadata":{"id":"-E6zD-rnZRK5"}},{"cell_type":"code","source":["def create_tfidf_features(news_df):\n","  # Create the TF-IDF vectorizer with preprocessing\n","  tfidf = TfidfVectorizer(strip_accents=None,\n","                          lowercase=True,\n","                          tokenizer=word_tokenize,\n","                          use_idf=True,\n","                          norm='l2',\n","                          smooth_idf=True,\n","                          stop_words='english',\n","                          max_df=0.5,\n","                          sublinear_tf=True)\n","\n","  # Fit and transform the combined column\n","  features = tfidf.fit_transform(news_df['Content'])\n","\n","  return features"],"metadata":{"id":"yeNbP3PKZSed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=5):\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get the indices of the relevant news in the features matrix (removing those read already)\n","  previously_read_indices = news_df[news_df['News ID'].isin(previously_read_article_ids)].index.tolist()\n","\n","  # # Get the indices of news articles read after the input timestamp (to exclude them in the next step)\n","  # timestamp = pd.to_datetime(timestamp)\n","  # future_article_indices = news_df[news_df['Release Date'] > timestamp].index.tolist()\n","\n","  all_indices = list(range(features.shape[0]))\n","  not_previously_read_indices = [idx for idx in all_indices if idx not in previously_read_indices]\n","  #not_previously_read_indices = [idx for idx in all_indices if idx not in previously_read_indices and idx not in future_article_indices]\n","\n","  # Aggregate the feature vectors of the read articles\n","  user_profile = np.asarray(features[previously_read_indices].sum(axis=0)/len(previously_read_indices))\n","\n","  # Calculate the similarity scores between the user profile and other articles\n","  similarity_scores = cosine_similarity(user_profile.reshape(1, -1), features[not_previously_read_indices]).flatten()\n","\n","  # Find the indices of the top 5 recommendations\n","  top_indices = similarity_scores.argsort()[-articles_k:][::-1]\n","\n","  # Get the top 5 recommended news articles\n","  final_recommended_article_ids = list(news_df.iloc[np.array(not_previously_read_indices)[top_indices].tolist(),[0]]['News ID'])\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"McxSOkg9aNrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def single_user_recommendations_pure_content_tfidf(user_id, timestamp, articles_k=5):\n","  # Create features\n","  features = create_tfidf_features(news_df)\n","\n","  #Run recommendations function\n","  final_recommended_article_ids = recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"YsnnInazun-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content_tfidf(user_ids_timestamps, articles_k=5):\n","  # Create features\n","  features = create_tfidf_features(news_df)\n","\n","  # create an empty dictionary to populate with recommendations\n","  user_recommendations_dict = {}\n","\n","  for user_id, timestamp in user_ids_timestamps:\n","    # run function for single users\n","    final_recommended_ids = recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=articles_k)\n","\n","    # create dictionary for final recommendations\n","    user_recommendations_dict[user_id] = final_recommended_ids\n","\n","  return user_recommendations_dict\n","\n"],"metadata":{"id":"vVJxoMeQqNDl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Functions that consolidate Pipeline"],"metadata":{"id":"xXy5hx8Wv5ey"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"R6cwLED9Md8h"}},{"cell_type":"code","source":["def single_user_recommendations_pure_content(user_id, timestamp, method='embeddings', articles_k=10):\n","  if method == 'embeddings':\n","    final_recommended_article_ids = single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=articles_k)\n","  elif method == 'tfidf':\n","    final_recommended_article_ids = single_user_recommendations_pure_content_tfidf(user_id, timestamp, articles_k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"cOwxEL0tv69D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multiple Users"],"metadata":{"id":"yaWlJxE3MgK-"}},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content(user_ids_timestamps, method='embeddings', articles_k=10):\n","  if method == 'embeddings':\n","    user_recommendations_dict = multiple_user_recommendations_pure_content_embeddings(user_ids_timestamps, articles_k=articles_k)\n","  elif method == 'tfidf':\n","    user_recommendations_dict = multiple_user_recommendations_pure_content_tfidf(user_ids_timestamps, articles_k=5)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"4TpBaA7Rw1Aw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Content Based Predictions"],"metadata":{"id":"JN8zOXd9ibDC"}},{"cell_type":"markdown","source":["## Test on Sample User"],"metadata":{"id":"_l6PcfVJcnDb"}},{"cell_type":"code","source":["final_recommended_ids = single_user_recommendations_pure_content(user_id='U13740', timestamp='2019-11-13 15:27:40', method='tfidf', articles_k=5)"],"metadata":{"id":"TfHu5PzHbHGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_recommended_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Z_76v_qcTF0","executionInfo":{"status":"ok","timestamp":1688036909476,"user_tz":-120,"elapsed":17,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"077d891e-b099-45f8-b709-cd5fbfc7a1b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['N59426', 'N34069', 'N61980', 'N59336', 'N628']"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Test on Multiple Users"],"metadata":{"id":"aKyhUQMscvpK"}},{"cell_type":"code","source":["# Select a subset of users of size k to test on\n","user_ids_timestamps = select_user_ids_timestamps(k=5)\n","\n","# Make recommednations for multiple users\n","final_recommended_ids_multiple = multiple_user_recommendations_pure_content(user_ids_timestamps, method='tfidf', articles_k=10)"],"metadata":{"id":"P-2wZqnxctLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View recommednations for multiple users\n","final_recommended_ids_multiple"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Phdq6yFLfr0t","executionInfo":{"status":"ok","timestamp":1688036925275,"user_tz":-120,"elapsed":16,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"3694862a-eb7e-412c-dc83-16dd6c40b92f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'U21593': ['N62353', 'N32663', 'N46293', 'N55619', 'N2224'],\n"," 'U10123': ['N13700', 'N42846', 'N16148', 'N30022', 'N39112'],\n"," 'U75630': ['N21336', 'N15209', 'N47120', 'N63515', 'N47615'],\n"," 'U44625': ['N1410', 'N33038', 'N6163', 'N35827', 'N61178'],\n"," 'U64800': ['N45575', 'N34904', 'N42252', 'N36327', 'N48603']}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Export file\n","with open('/content/drive/MyDrive/Group_19/01.Dataset/prediction_content.json', 'w') as json_file:\n","    json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"azX1d-QxmcpC"},"execution_count":null,"outputs":[]}]}