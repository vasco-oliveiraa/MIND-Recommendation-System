{"cells":[{"cell_type":"markdown","source":["To do:\n","- Remove dropping NAs from user_similarity_df in fetch_similar_users()"],"metadata":{"id":"T7DZ0Cwb3-F4"}},{"cell_type":"markdown","metadata":{"id":"oSkyK2IsuqNl"},"source":["## Set Up"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1688041926851,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"},"user_tz":-120},"id":"DnZAe-CVuqNp","outputId":"c917fbf3-22e4-4e25-d661-6d8833460944"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import datetime\n","from scipy.sparse import csc_matrix\n","from scipy.sparse import load_npz\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models import KeyedVectors\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.corpus import stopwords\n","import heapq\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","pd.set_option('display.max_colwidth', None)\n","import json\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["# Function to select multiple user IDs & timestamps for evaluation\n","def select_user_ids_timestamps(k=5):\n","  # Select the top 10 rows\n","  filtered_behaviors_df = behaviors_df.tail(k)\n","\n","  # Create a list of tuples containing values from columns 'a' and 'b'\n","  user_ids_timestamps = [(row['User ID'], row['Timestamp']) for _, row in filtered_behaviors_df.iterrows()]\n","\n","  return user_ids_timestamps"],"metadata":{"id":"7mrNpiOtNvU0","executionInfo":{"status":"ok","timestamp":1688043233531,"user_tz":-120,"elapsed":235,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85PgkxPTuqNr"},"source":["## Import Data\n","\n"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8Ww33DFuqNr","executionInfo":{"status":"ok","timestamp":1688045161220,"user_tz":-120,"elapsed":89043,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"c9e22fde-38bf-4de3-9cdd-e167313a25df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# # import unique behaviours df\n","# unique_user_behaviors = pd.read_csv(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/unique_user_behaviors_train.csv\")\n","\n","# # Keep all columns in a separate df\n","# unique_user_behaviors_with_time = unique_user_behaviors.copy()\n","\n","# import news df\n","news_df = pd.read_pickle(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/news.pkl\")\n","\n","# # import user article sparse matrix\n","# sparse_user_matrix = load_npz(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/unique_user_sparse.npz\")\n","\n","# Load pre-trained Google News Word2Vec model\n","model_path = \"/content/drive/MyDrive/Group_19/01.Dataset/GoogleNews-vectors-negative300.bin\"\n","google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","\n","###########\n","# import behaviors df\n","behaviors_df = pd.read_pickle(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/behaviors.pkl\")"]},{"cell_type":"markdown","source":["# Collaborative + Content Model"],"metadata":{"id":"hxB0QzDoiQKo"}},{"cell_type":"markdown","metadata":{"id":"4CKAv7qduqNt"},"source":["## Define Functions for User to User Collaborative Filtering"]},{"cell_type":"code","source":["def fetch_similar_users(user_id, timestamp, k=5):\n","  # Get IDs in user's history\n","  presiouvly_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_user_vector = news_df.loc[news_df['News ID'].isin(presiouvly_read_article_ids), 'Average Vector'].mean()\n","\n","  # Create a copy of behaviours_df\n","  user_similarity_df = behaviors_df.copy()\n","  user_similarity_df = user_similarity_df.dropna()\n","\n","  # Filter out input user from user_similarity_df\n","  user_similarity_df = user_similarity_df.loc[user_similarity_df['User ID'] != user_id]\n","\n","  # Drop duplicate users\n","  user_similarity_df = user_similarity_df.drop_duplicates(subset=['User ID', 'History & Impressions'])\n","\n","  # Compute cosine similarity between average_news_vector and each unread news article\n","  user_similarity_df['Similarity'] = user_similarity_df['Average Vector'].apply(lambda x: cosine_similarity([average_user_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  user_similarity_df = user_similarity_df.sort_values(by='Similarity', ascending=False).head(k)\n","\n","  # Get similar users\n","  similar_users_timestamps = [(row['User ID'], row['Timestamp']) for _, row in user_similarity_df.iterrows()]\n","\n","  return similar_users_timestamps"],"metadata":{"id":"dNNlt9zUzvTd","executionInfo":{"status":"ok","timestamp":1688041936144,"user_tz":-120,"elapsed":225,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps):\n","  # Filter behaviors df for similar users & timestamps\n","  similar_users_df = behaviors_df[behaviors_df[['User ID', 'Timestamp']].apply(tuple, axis=1).isin(similar_users_timestamps)]\n","\n","  # Initialize list to store relevant article IDs\n","  recommended_article_ids = []\n","\n","  # Iterate over the rows of the DataFrame\n","  for index, row in similar_users_df.iterrows():\n","    # Split the text into words and add them to the word_list\n","    recommended_article_ids.extend(row['History & Impressions'].split())\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Remove any already read articles from the recommended articles\n","  recommended_article_ids = list(set([id for id in recommended_article_ids if id not in previously_read_article_ids]))\n","\n","  return recommended_article_ids"],"metadata":{"id":"LfVNjJrm3Bmb","executionInfo":{"status":"ok","timestamp":1688041937872,"user_tz":-120,"elapsed":243,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpCRATmLuqNu"},"source":["## Define Functions for Content Based Filtering - Post Collaborative Filtering"]},{"cell_type":"markdown","source":["### Word2Vec"],"metadata":{"id":"2CTye4a4O431"}},{"cell_type":"code","source":["def create_previously_read_content(user_id, timestamp):\n","  '''Inputs:\n","  previously_read_article_ids: list of article IDs previously read by a given user, provided by recommend_articles_collaborative function\n","  news_df: clean news dataframe imported from drive\n","\n","  Outputs:\n","  previously_read_content: list of words in all of the articles that were previously read by a given user\n","  '''\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # create filtered news df for articles previously read by a user\n","  previously_read_articles_df = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), ['News ID', 'Content']]\n","\n","  # create list of words containing all content words from rpeviously read articles\n","  previously_read_content = ' '.join(previously_read_articles_df['Content']).split()\n","\n","  return previously_read_content"],"metadata":{"id":"Gue3O2a-OVw5","executionInfo":{"status":"ok","timestamp":1688045039145,"user_tz":-120,"elapsed":482,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["def create_recommended_content(recommended_article_ids):\n","    '''Inputs:\n","    recommended_article_ids: list of article IDs recommended by the recommend_articles_collaborative function\n","    news_df: clean news dataframe imported from drive\n","\n","    Outputs:\n","    recommended_content: dictionary with recommended article_ids as keys and list of words in article content as values\n","    '''\n","    # create filtered news df for recommended articles\n","    recommended_articles_df = news_df.loc[news_df['News ID'].isin(recommended_article_ids), ['News ID', 'Content']]\n","\n","    # Create an empty dictionary\n","    recommended_content = {}\n","\n","    # Iterate over the rows of the DataFrame\n","    for row in recommended_articles_df.itertuples(index=False):\n","        news_id = row[0]\n","        content = row[1]\n","\n","        # Split the content string into words\n","        words = content.split()\n","\n","        # Add the key-value pair to the dictionary\n","        recommended_content[news_id] = words\n","\n","    return recommended_content"],"metadata":{"id":"ffN4kMLqQQZy","executionInfo":{"status":"ok","timestamp":1688045040277,"user_tz":-120,"elapsed":297,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# function to process inputs and identify most relevant article (based on cosine similarity)\n","def recommend_articles_content_w2v(previously_read_content, recommended_content, k=5):\n","    '''\n","    previously_read_content: list of content words in a user's previously read articles obtained from create_previously_read_content function\n","    recommended_content: dictionary with article_ids as keys and list of words in article content as values obtained from create_recommended_content function\n","    '''\n","    # Remove words not used in model training from interests & articles\n","    previously_read_content = [content for content in previously_read_content if content in list(google_model.key_to_index.keys())]\n","    recommended_content = {key: [word for word in content if word.lower() in google_model.key_to_index] for key, content in recommended_content.items()}\n","\n","    # create empty dictionary to store similarity scores\n","    similarity_scores = {}\n","\n","    # iterate through articles dictionary\n","    for news_id, content in recommended_content.items():\n","        # calculate cosine similarity between the list of keywords of an article and the list of user interests\n","        similarity_score = google_model.n_similarity(previously_read_content, content)\n","\n","        similarity_scores[news_id] = similarity_score\n","\n","    # Get the top n 'News ID' with the highest values\n","    final_recommended_article_ids = heapq.nlargest(k, similarity_scores, key=similarity_scores.get)\n","\n","    return final_recommended_article_ids"],"metadata":{"id":"PSxqGiURQ4-p","executionInfo":{"status":"ok","timestamp":1688045041547,"user_tz":-120,"elapsed":212,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["### Embeddings"],"metadata":{"id":"iGe6OANaSNJn"}},{"cell_type":"code","source":["def get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=5):\n","\n","  # create filtered_news_df based on recommended articles from collaborative based filtering\n","  filtered_news_df = news_df.loc[news_df['News ID'].isin(recommended_article_ids)]\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_news_vector = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), 'Average Vector'].mean()\n","\n","  # Filter news_df to exlcude articles in user history\n","  filtered_news_df = filtered_news_df.loc[~filtered_news_df['News ID'].isin(previously_read_article_ids)]\n","\n","  # Compute cosine similarity between average_news_vector and each unread news article\n","  filtered_news_df['Similarity'] = filtered_news_df['Average Vector'].apply(lambda x: cosine_similarity([average_news_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  filtered_news_df = filtered_news_df.sort_values(by='Similarity', ascending=False)\n","\n","  #select top k articles\n","  top_k_recommended_article_ids = filtered_news_df.head(k)['News ID'].tolist()\n","\n","  return top_k_recommended_article_ids"],"metadata":{"id":"CaBGvL5pKTk4","executionInfo":{"status":"ok","timestamp":1688041943048,"user_tz":-120,"elapsed":229,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFRePjiQXuvB"},"source":["### TFIDF"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ykRmF59IYNg1","executionInfo":{"status":"ok","timestamp":1688041946857,"user_tz":-120,"elapsed":259,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"outputs":[],"source":["def create_tfidf_features(news_df):\n","  # Create the TF-IDF vectorizer with preprocessing\n","  tfidf = TfidfVectorizer(strip_accents=None,\n","                          lowercase=True,\n","                          tokenizer=word_tokenize,\n","                          use_idf=True,\n","                          norm='l2',\n","                          smooth_idf=True,\n","                          stop_words='english',\n","                          max_df=0.5,\n","                          sublinear_tf=True)\n","\n","  # Fit and transform the combined column\n","  features = tfidf.fit_transform(news_df['Content'])\n","\n","  return features"]},{"cell_type":"code","source":["def recommend_articles_content_tfidf(user_id, timestamp, recommended_articles_ids, features, k=5):\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","    )\n","\n","  # Get the indices of the relevant news in the features matrix (removing those read already)\n","  news_indices = news_df[news_df['News ID'].isin(previously_read_article_ids)].index.tolist()\n","\n","  # Get the indices of the recommended news in the features matrix\n","  recomended_articles_indices = news_df[news_df['News ID'].isin(recommended_articles_ids)].index.tolist()\n","\n","  all_indices = list(range(features.shape[0]))\n","  not_news_indices = [idx for idx in all_indices if idx in recomended_articles_indices]\n","\n","  # Aggregate the feature vectors of the read articles\n","  user_profile = np.asarray(features[news_indices].sum(axis=0)/len(news_indices))\n","\n","  # Calculate the similarity scores between the user profile and other articles\n","  similarity_scores = cosine_similarity(user_profile.reshape(1, -1), features[not_news_indices]).flatten()\n","\n","  # Find the indices of the top 5 recommendations\n","  top_indices = similarity_scores.argsort()[-k:][::-1]\n","\n","  # Get the top 5 recommended news articles\n","  final_recommended_article_ids = list(news_df.iloc[np.array(not_news_indices)[top_indices].tolist(),[0]]['News ID'])\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"WgTWlaorMhVj","executionInfo":{"status":"ok","timestamp":1688041947729,"user_tz":-120,"elapsed":1,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9v6AABOuqNv"},"source":["## Define Functions that consolidate Pipeline"]},{"cell_type":"markdown","metadata":{"id":"cARuG1m6uqNv"},"source":["### Single User"]},{"cell_type":"code","source":["def single_user_recommendations_combined(user_id, timestamp, method='word2vec', similar_user_k=5, articles_k=5):\n","\n","  # Get user ID & time stamp of similar user interactions\n","  similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n","\n","  # Get article IDs read by similar user interactions\n","  recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n","\n","  # Apply embeddings or TFIDF methodology\n","\n","  if method=='word2vec':\n","    # create previously read content list\n","    previously_read_content = create_previously_read_content(user_id, timestamp)\n","    # create recommended content dictionary\n","    recommended_content = create_recommended_content(recommended_article_ids)\n","    # find final similar articles\n","    final_recommended_article_ids = recommend_articles_content_w2v(previously_read_content, recommended_content, k=articles_k)\n","\n","  elif method == 'embeddings':\n","    # find final similar articles\n","    final_recommended_article_ids = get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=articles_k)\n","\n","  elif method == 'tfidf':\n","    # create features\n","    features = create_tfidf_features(news_df)\n","    # find final similar articles\n","    final_recommended_article_ids = recommend_articles_content_tfidf(user_id, timestamp, recommended_article_ids, features, k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"1kFAbfvqJMEy","executionInfo":{"status":"ok","timestamp":1688045044057,"user_tz":-120,"elapsed":230,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLdgvwoQuqNv"},"source":["### Multiple Users"]},{"cell_type":"code","source":["def multiple_user_recommendations_combined(user_ids_timestamps, method='tfidf', similar_user_k=5, articles_k=5):\n","\n","  # Create empty dictionary to store recommendations\n","  user_recommendations_dict = {}\n","\n","  # If method is tfidf create features\n","  if method == 'tfidf':\n","    features = create_tfidf_features(news_df)\n","\n","  # Keep track of how many iterations have run\n","  counter = 0\n","\n","  # Iterate over users & timestamps\n","  for user_id, timestamp in user_ids_timestamps:\n","    # Update counter\n","    counter += 1\n","    # Get user ID & time stamp of similar user interactions\n","    similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n","\n","    # Get article IDs read by similar user interactions\n","    recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n","\n","    if method=='word2vec':\n","      # create previously read content list\n","      previously_read_content = create_previously_read_content(user_id, timestamp)\n","      # create recommended content dictionary\n","      recommended_content = create_recommended_content(recommended_article_ids)\n","      # find final similar articles\n","      final_recommended_article_ids = recommend_articles_content_w2v(previously_read_content, recommended_content, k=articles_k)\n","\n","    # Apply embeddings or TFIDF methodology\n","    elif method == 'embeddings':\n","      final_recommended_ids = get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=articles_k)\n","      user_recommendations_dict[user_id] = final_recommended_ids\n","\n","    elif method == 'tfidf':\n","      final_recommended_ids = recommend_articles_content_tfidf(user_id, timestamp, recommended_article_ids, features, k=articles_k)\n","      user_recommendations_dict[user_id] = final_recommended_ids\n","      print(counter)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"3lHdtvGIG_pY","executionInfo":{"status":"ok","timestamp":1688045401053,"user_tz":-120,"elapsed":214,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4FmCwhYuqNw"},"source":["## Test on Sample User"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":74585,"status":"ok","timestamp":1688045296529,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"},"user_tz":-120},"id":"xJkO1IPEuqNw"},"outputs":[],"source":[" # Run recommender system\n","final_recommended_ids = single_user_recommendations_combined('U13740', '2019-11-13 15:27:40', method='word2vec', similar_user_k=5, articles_k=5)"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1688045310977,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"},"user_tz":-120},"id":"omG4By0ZuqNw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"115f100b-91c6-4212-fd59-53efc4724fa6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['N9674', 'N24691', 'N25635', 'N64273', 'N21547']"]},"metadata":{},"execution_count":53}],"source":["# view recommendations - avg vec\n","final_recommended_ids"]},{"cell_type":"markdown","metadata":{"id":"DBQLjSrBuqNw"},"source":["## Test on Multiple Users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWGIbcoguqNx"},"outputs":[],"source":["# Select a subset of users of size k to test on\n","user_ids_timestamps = select_user_ids_timestamps(k=5)\n","\n","# Run recommender system\n","final_recommended_ids_multiple = multiple_user_recommendations_combined(user_ids_timestamps, method='word2vec', similar_user_k=5, articles_k=5)"]},{"cell_type":"code","source":["final_recommended_ids_multiple"],"metadata":{"id":"fQUKyYT5s_J0","executionInfo":{"status":"ok","timestamp":1688043510258,"user_tz":-120,"elapsed":22,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"720ad607-ab6e-498f-9e63-297542bb384c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'U21593': ['N42781', 'N59704', 'N30665', 'N16655', 'N46039'],\n"," 'U10123': ['N8448', 'N16344', 'N27612', 'N41172', 'N10843'],\n"," 'U75630': ['N16384', 'N37304', 'N61352', 'N64305', 'N52236'],\n"," 'U44625': ['N10928', 'N4255', 'N58860', 'N63302', 'N11523'],\n"," 'U64800': ['N17303', 'N27951', 'N287', 'N44021', 'N35170']}"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["### Export recommendations for evaluation"],"metadata":{"id":"4sfVXusR64VY"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Group_19/01.Dataset/final_recommended_ids_multiple.json', 'w') as json_file:\n","    json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"PRX2qkFW3pBN"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["iGe6OANaSNJn","sFRePjiQXuvB"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}