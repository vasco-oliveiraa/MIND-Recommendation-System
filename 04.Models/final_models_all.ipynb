{"cells":[{"cell_type":"markdown","source":["# README\n","\n","This notebook contains all model iterations created for this project starting from a simple random recommender to outr final model, a collaborative and content based filtering recommender system.\n","\n","To use any of the models to generate recommendations for a given user ID at a given timestamp (eg. \"2019-11-13 15:27:40\"), or for multiple user IDs at different timestamps follow the steps below:\n","\n","1. Run notebook from start to end of 'Model 3: Collaborative & Content Based Recommender'\n","\n","2. Open 'Make Predictions' section\n","  - If testing on multiple users select number of test users\n","\n","3. Open subheading of model you want to use\n","\n","4. Select single user or multi user recommendations\n","\n","5. Run chosen model\n","\n","6. Optionally uncomment code to export results to json file"],"metadata":{"id":"DA3oovmMJR16"}},{"cell_type":"markdown","metadata":{"id":"oSkyK2IsuqNl"},"source":["# Set Up"]},{"cell_type":"markdown","source":["## Install Required Libraries"],"metadata":{"id":"3QjjPJ31x55z"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2787,"status":"ok","timestamp":1688801800004,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"},"user_tz":-120},"id":"DnZAe-CVuqNp","outputId":"61b56de1-a664-482e-fb45-5e1eeb37e029"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import datetime\n","import scipy.sparse as sp\n","from scipy.sparse import csc_matrix\n","from scipy.sparse import load_npz\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models import KeyedVectors\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.corpus import stopwords\n","import heapq\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","pd.set_option('display.max_colwidth', None)\n","import json\n","import ast\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import urllib.request\n","import os\n","from datetime import timedelta\n","from scipy.spatial.distance import cosine\n","import itertools\n","import random"]},{"cell_type":"markdown","metadata":{"id":"85PgkxPTuqNr"},"source":["## Import Data\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8Ww33DFuqNr","executionInfo":{"status":"ok","timestamp":1688804013070,"user_tz":-120,"elapsed":5697,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"3e386a83-3e82-492e-f655-be1195028df8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# import behaviors df\n","behaviors_df = pd.read_pickle(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/behaviors.pkl\")\n","\n","# import news df\n","news_df = pd.read_pickle(\"/content/drive/MyDrive/Group_19/01.Dataset/Small/Clean/Train/news.pkl\")\n","\n","# # Load pre-trained Google News Word2Vec model\n","# model_path = \"/content/drive/MyDrive/Group_19/01.Dataset/GoogleNews-vectors-negative300.bin\"\n","# google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","\n","# # Load pre-trained Glove Word2Vec model\n","# model_path = \"/content/drive/MyDrive/Group_19/01.Dataset/glove.6B.300d.txt\"\n","# glove_model = {}\n","\n","# with open(model_path, 'r', encoding='utf8') as file:\n","#     for line in file:\n","#         parts = line.split()\n","#         word = parts[0]\n","#         embedding = np.array([float(val) for val in parts[1:]])\n","#         glove_model[word] = embedding"]},{"cell_type":"code","source":["behaviors_df.loc[behaviors_df['User ID']=='U13740']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Mk3dKCoykCbo","executionInfo":{"status":"ok","timestamp":1688804034808,"user_tz":-120,"elapsed":240,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"0e15a730-48cf-4800-c47a-3740584ed50b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       User ID           Timestamp  \\\n","0       U13740 2019-11-11 09:05:58   \n","35262   U13740 2019-11-09 05:59:43   \n","154836  U13740 2019-11-13 15:27:40   \n","\n","                                                               History  \\\n","0       N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694   \n","35262   N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694   \n","154836  N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694   \n","\n","       Impressions  \\\n","0           N55689   \n","35262       N28910   \n","154836      N58133   \n","\n","                                                        History & Impressions  \\\n","0       N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N55689   \n","35262   N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N28910   \n","154836  N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N58133   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Average Vector  \n","0       [-0.014606062493754818, -0.02841637492969335, 0.02371981330357809, 0.009556050663624305, -0.0422950266549711, -0.012634033702098712, 0.025006863912623013, -0.002610287599750112, 0.007933776577744732, 0.0057190535123700665, 0.0491532812601282, 0.05749945489872381, 0.026925346099389746, -0.014982412822799274, -0.025055876820228112, -0.023746189567295, 0.005080339706172858, 0.008010757221064476, -0.04396915791593488, 0.002112705376362015, -0.06642412956471191, -0.012746648982221035, -0.0022455996219859233, 0.0006853153529116356, -0.0012351444187181488, -0.016091391157134573, 0.002439112567173429, -0.006423315420792992, 0.01211253476254524, 0.009400782066094678, -0.014237181864117415, 0.019026322500717258, 0.04758785196220639, -0.04587253953831038, 0.024527962524958157, -0.03714662646084465, -0.05322331354597981, -0.035100688131982125, -0.004473486433301354, 0.037951936227451084, -0.025963429143521833, 0.03638469596056955, -0.009913495753708366, 0.01663233791264424, -0.018381362213831222, -0.029326694539909253, -0.006303412268038785, 0.039395503069485215, -0.00180327118308573, 0.0072839031204378485, -0.0231982012384921, -0.01576067722869785, 0.035454009785580574, 0.05028668191649095, 0.014613864029587953, 0.03431599349521232, -0.0028338826792617153, -0.012185012552739315, -0.02298737854135876, 0.009832405998956071, -0.004609509098362196, 0.03469953616090543, 0.011528941747851795, 0.01476928494028599, 0.016092007446671122, 0.026100055105888604, -0.022858431022648507, -0.03895301730120605, 0.027643572927700626, -0.014615578402528157, -0.045407649509863146, -0.0055726218631066025, 0.04967636279998334, 0.0429824294916797, -0.036944973509377414, -0.00439484496218269, -0.009726584098703457, 0.044619129220500875, -0.00026324803551897415, 0.04958850295594698, 0.022808497740811864, -0.023784744676203402, -0.0053312269170054435, 0.04307054397984106, 0.046043928934022996, -0.000992646750708694, 0.013062171327589151, -0.021695856995925966, -0.011250172181620416, -0.009029016311564381, -0.004436389195224033, 0.0404441875799851, 0.022881511630580217, -0.020203295700649174, 0.013582978203417778, -0.05494556589388144, 0.018376246195553475, -0.035659490832620135, -0.002349582923667594, 0.03061314961582341]  \n","35262        [-0.010981766104865925, -0.029624159991421744, 0.027244411359133643, 0.007351641373500849, -0.03657101727225505, -0.01646845947370365, 0.020065892245956347, -0.004404096982466162, 0.008678827195028683, 0.016270772709900932, 0.04940365916136276, 0.05628198906539047, 0.028854384617908263, -0.01480641461292273, -0.024724894165907126, -0.022788770740134502, -0.0004018167753086242, 0.010598921572916328, -0.03958575461346573, -0.0005292904878355153, -0.06257701552150205, -0.014289750587159306, -0.00433521780099827, 0.005783365352911636, -0.0026956031532860505, -0.016415801527504943, 0.00023985124001293516, -0.00896253193313867, 0.0066187410279773375, 0.013842476047576158, -0.017869161061648278, 0.011976404506890095, 0.043247099739984175, -0.04065141793337211, 0.017933288265698896, -0.03066350670775823, -0.04755645397807858, -0.041275204767784596, -0.014060182544412467, 0.03876759048671034, -0.029651643742287258, 0.03554753592970535, -0.014748052975930587, 0.012888706215113371, -0.022387584374325052, -0.03326139083620555, -0.009666625570507922, 0.037445462822571635, -0.0064088256892585686, 0.010258421299450194, -0.022624137966887163, -0.011647145068204023, 0.028387276205333666, 0.045820876330071196, 0.012354245819711408, 0.026114436519903684, -0.006008908543459245, -0.014299091688541788, -0.02168033903518592, -0.0005057054825254092, -0.011283417339102935, 0.0435747490930042, 0.0117384829206913, 0.021689618736582282, 0.019088498033090876, 0.0161309826367528, -0.0264907162078337, -0.034088050325897415, 0.020029997649922852, -0.01773368750746643, -0.0489901375654187, -0.005341304208785616, 0.048350196874057415, 0.04895404801019822, -0.03381068810814285, -0.008061452801688863, -0.014573398820925676, 0.04416542101062433, 0.0044459332607773225, 0.05293511236952723, 0.023569952864268655, -0.021495212886079945, -0.003073252626881987, 0.04583455756008797, 0.04075015325501065, -0.0066736016889803, 0.014034017932527426, -0.01796681258234572, -0.0128409573668056, -0.014999166836255743, -0.007889771448310452, 0.0348101114071456, 0.02089525169230861, -0.02864211443521707, 0.007089298481195554, -0.0550092576840049, 0.023571314559751005, -0.04006091178941026, -0.0015419003310750022, 0.02903490115903329]  \n","154836   [-0.013295162493754818, -0.03683597492969335, 0.027015733303578086, 0.008463010663624305, -0.03511638665497109, -0.016911013702098715, 0.020322843912623012, -0.008159407599750111, 0.01571723657774473, 0.013127013512370066, 0.05795892126012819, 0.06206919489872381, 0.033033226099389745, -0.013415112822799271, -0.030497736820228116, -0.018950689567295002, 0.005552539706172858, 0.016516357221064478, -0.03863017791593487, 0.0008932453763620153, -0.06501572956471191, -0.018877328982221032, 0.0006657603780140769, 0.0015387953529116358, -0.006922904418718148, -0.018790971157134576, 0.001702832567173429, -0.005637895420792993, 0.00877015476254524, 0.01246232206609468, -0.021662841864117415, 0.01861642250071726, 0.048091811962206396, -0.04038821953831038, 0.019236662524958155, -0.033857706460844655, -0.05362673354597981, -0.04045744813198213, -0.013833066433301355, 0.03666595622745108, -0.03623120914352183, 0.036570475960569546, -0.017925435753708367, 0.015333177912644236, -0.025425042213831222, -0.031651554539909255, -0.016167652268038783, 0.040379603069485216, -0.0047002311830857315, 0.006582243120437847, -0.024645901238492102, -0.01519541722869785, 0.03293450978558058, 0.050069841916490956, 0.019057964029587954, 0.03126317349521232, -0.006587762679261716, -0.014045792552739318, -0.02929505854135876, 0.004411325998956071, -0.007994829098362194, 0.03854827616090543, 0.009039741747851795, 0.025962924940285986, 0.018530867446671123, 0.011351295105888604, -0.02661867102264851, -0.034776197301206055, 0.01157397292770063, -0.016911378402528158, -0.04996402950986314, -0.012486421863106603, 0.049396682799983334, 0.0448499694916797, -0.030522153509377415, -0.0032535649621826894, -0.013438504098703456, 0.05246936922050087, -0.0006849080355189735, 0.05603364295594698, 0.026106577740811866, -0.0202511046762034, -0.000903206917005444, 0.04516004397984105, 0.044757908934023, -0.005157186750708695, 0.01629351132758915, -0.02104193699592597, -0.021222812181620414, -0.009010796311564382, -0.009542429195224035, 0.035533747579985106, 0.020696491630580217, -0.02716111570064917, 0.0062255582034177765, -0.054727845893881445, 0.014750446195553477, -0.04270347083262013, -0.0009777229236675937, 0.026759389615823415]  "],"text/html":["\n","  <div id=\"df-3d2d7be9-d562-49f1-90ee-c77314b79307\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Timestamp</th>\n","      <th>History</th>\n","      <th>Impressions</th>\n","      <th>History &amp; Impressions</th>\n","      <th>Average Vector</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>U13740</td>\n","      <td>2019-11-11 09:05:58</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694</td>\n","      <td>N55689</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N55689</td>\n","      <td>[-0.014606062493754818, -0.02841637492969335, 0.02371981330357809, 0.009556050663624305, -0.0422950266549711, -0.012634033702098712, 0.025006863912623013, -0.002610287599750112, 0.007933776577744732, 0.0057190535123700665, 0.0491532812601282, 0.05749945489872381, 0.026925346099389746, -0.014982412822799274, -0.025055876820228112, -0.023746189567295, 0.005080339706172858, 0.008010757221064476, -0.04396915791593488, 0.002112705376362015, -0.06642412956471191, -0.012746648982221035, -0.0022455996219859233, 0.0006853153529116356, -0.0012351444187181488, -0.016091391157134573, 0.002439112567173429, -0.006423315420792992, 0.01211253476254524, 0.009400782066094678, -0.014237181864117415, 0.019026322500717258, 0.04758785196220639, -0.04587253953831038, 0.024527962524958157, -0.03714662646084465, -0.05322331354597981, -0.035100688131982125, -0.004473486433301354, 0.037951936227451084, -0.025963429143521833, 0.03638469596056955, -0.009913495753708366, 0.01663233791264424, -0.018381362213831222, -0.029326694539909253, -0.006303412268038785, 0.039395503069485215, -0.00180327118308573, 0.0072839031204378485, -0.0231982012384921, -0.01576067722869785, 0.035454009785580574, 0.05028668191649095, 0.014613864029587953, 0.03431599349521232, -0.0028338826792617153, -0.012185012552739315, -0.02298737854135876, 0.009832405998956071, -0.004609509098362196, 0.03469953616090543, 0.011528941747851795, 0.01476928494028599, 0.016092007446671122, 0.026100055105888604, -0.022858431022648507, -0.03895301730120605, 0.027643572927700626, -0.014615578402528157, -0.045407649509863146, -0.0055726218631066025, 0.04967636279998334, 0.0429824294916797, -0.036944973509377414, -0.00439484496218269, -0.009726584098703457, 0.044619129220500875, -0.00026324803551897415, 0.04958850295594698, 0.022808497740811864, -0.023784744676203402, -0.0053312269170054435, 0.04307054397984106, 0.046043928934022996, -0.000992646750708694, 0.013062171327589151, -0.021695856995925966, -0.011250172181620416, -0.009029016311564381, -0.004436389195224033, 0.0404441875799851, 0.022881511630580217, -0.020203295700649174, 0.013582978203417778, -0.05494556589388144, 0.018376246195553475, -0.035659490832620135, -0.002349582923667594, 0.03061314961582341]</td>\n","    </tr>\n","    <tr>\n","      <th>35262</th>\n","      <td>U13740</td>\n","      <td>2019-11-09 05:59:43</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694</td>\n","      <td>N28910</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N28910</td>\n","      <td>[-0.010981766104865925, -0.029624159991421744, 0.027244411359133643, 0.007351641373500849, -0.03657101727225505, -0.01646845947370365, 0.020065892245956347, -0.004404096982466162, 0.008678827195028683, 0.016270772709900932, 0.04940365916136276, 0.05628198906539047, 0.028854384617908263, -0.01480641461292273, -0.024724894165907126, -0.022788770740134502, -0.0004018167753086242, 0.010598921572916328, -0.03958575461346573, -0.0005292904878355153, -0.06257701552150205, -0.014289750587159306, -0.00433521780099827, 0.005783365352911636, -0.0026956031532860505, -0.016415801527504943, 0.00023985124001293516, -0.00896253193313867, 0.0066187410279773375, 0.013842476047576158, -0.017869161061648278, 0.011976404506890095, 0.043247099739984175, -0.04065141793337211, 0.017933288265698896, -0.03066350670775823, -0.04755645397807858, -0.041275204767784596, -0.014060182544412467, 0.03876759048671034, -0.029651643742287258, 0.03554753592970535, -0.014748052975930587, 0.012888706215113371, -0.022387584374325052, -0.03326139083620555, -0.009666625570507922, 0.037445462822571635, -0.0064088256892585686, 0.010258421299450194, -0.022624137966887163, -0.011647145068204023, 0.028387276205333666, 0.045820876330071196, 0.012354245819711408, 0.026114436519903684, -0.006008908543459245, -0.014299091688541788, -0.02168033903518592, -0.0005057054825254092, -0.011283417339102935, 0.0435747490930042, 0.0117384829206913, 0.021689618736582282, 0.019088498033090876, 0.0161309826367528, -0.0264907162078337, -0.034088050325897415, 0.020029997649922852, -0.01773368750746643, -0.0489901375654187, -0.005341304208785616, 0.048350196874057415, 0.04895404801019822, -0.03381068810814285, -0.008061452801688863, -0.014573398820925676, 0.04416542101062433, 0.0044459332607773225, 0.05293511236952723, 0.023569952864268655, -0.021495212886079945, -0.003073252626881987, 0.04583455756008797, 0.04075015325501065, -0.0066736016889803, 0.014034017932527426, -0.01796681258234572, -0.0128409573668056, -0.014999166836255743, -0.007889771448310452, 0.0348101114071456, 0.02089525169230861, -0.02864211443521707, 0.007089298481195554, -0.0550092576840049, 0.023571314559751005, -0.04006091178941026, -0.0015419003310750022, 0.02903490115903329]</td>\n","    </tr>\n","    <tr>\n","      <th>154836</th>\n","      <td>U13740</td>\n","      <td>2019-11-13 15:27:40</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694</td>\n","      <td>N58133</td>\n","      <td>N42782 N18445 N55189 N19347 N45794 N10414 N63302 N31801 N34694 N58133</td>\n","      <td>[-0.013295162493754818, -0.03683597492969335, 0.027015733303578086, 0.008463010663624305, -0.03511638665497109, -0.016911013702098715, 0.020322843912623012, -0.008159407599750111, 0.01571723657774473, 0.013127013512370066, 0.05795892126012819, 0.06206919489872381, 0.033033226099389745, -0.013415112822799271, -0.030497736820228116, -0.018950689567295002, 0.005552539706172858, 0.016516357221064478, -0.03863017791593487, 0.0008932453763620153, -0.06501572956471191, -0.018877328982221032, 0.0006657603780140769, 0.0015387953529116358, -0.006922904418718148, -0.018790971157134576, 0.001702832567173429, -0.005637895420792993, 0.00877015476254524, 0.01246232206609468, -0.021662841864117415, 0.01861642250071726, 0.048091811962206396, -0.04038821953831038, 0.019236662524958155, -0.033857706460844655, -0.05362673354597981, -0.04045744813198213, -0.013833066433301355, 0.03666595622745108, -0.03623120914352183, 0.036570475960569546, -0.017925435753708367, 0.015333177912644236, -0.025425042213831222, -0.031651554539909255, -0.016167652268038783, 0.040379603069485216, -0.0047002311830857315, 0.006582243120437847, -0.024645901238492102, -0.01519541722869785, 0.03293450978558058, 0.050069841916490956, 0.019057964029587954, 0.03126317349521232, -0.006587762679261716, -0.014045792552739318, -0.02929505854135876, 0.004411325998956071, -0.007994829098362194, 0.03854827616090543, 0.009039741747851795, 0.025962924940285986, 0.018530867446671123, 0.011351295105888604, -0.02661867102264851, -0.034776197301206055, 0.01157397292770063, -0.016911378402528158, -0.04996402950986314, -0.012486421863106603, 0.049396682799983334, 0.0448499694916797, -0.030522153509377415, -0.0032535649621826894, -0.013438504098703456, 0.05246936922050087, -0.0006849080355189735, 0.05603364295594698, 0.026106577740811866, -0.0202511046762034, -0.000903206917005444, 0.04516004397984105, 0.044757908934023, -0.005157186750708695, 0.01629351132758915, -0.02104193699592597, -0.021222812181620414, -0.009010796311564382, -0.009542429195224035, 0.035533747579985106, 0.020696491630580217, -0.02716111570064917, 0.0062255582034177765, -0.054727845893881445, 0.014750446195553477, -0.04270347083262013, -0.0009777229236675937, 0.026759389615823415]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d2d7be9-d562-49f1-90ee-c77314b79307')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3d2d7be9-d562-49f1-90ee-c77314b79307 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3d2d7be9-d562-49f1-90ee-c77314b79307');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## Create Function for Evaluation on Multiple Users"],"metadata":{"id":"VE6DDfCFVRMX"}},{"cell_type":"code","source":["# Function to select multiple user IDs & timestamps for evaluation\n","def select_user_ids_timestamps(minimum_history=5, minimum_impressions=1, k=5):\n","\n","  # Convert timestamps to string\n","  behaviors_df['Timestamp'] = behaviors_df['Timestamp'].astype(str)\n","\n","  # Select minimum number of articles in history\n","  filtered_behaviors_df = behaviors_df[behaviors_df['History'].str.split().str.len() >= minimum_history]\n","\n","  # Select minimum number of articles in impressions\n","  filtered_behaviors_df = filtered_behaviors_df[filtered_behaviors_df['Impressions'].str.split().str.len() >= minimum_impressions]\n","\n","  # Select the top 10 rows\n","  filtered_behaviors_df = filtered_behaviors_df.tail(k)\n","\n","  # Create a list of tuples containing values from columns 'a' and 'b'\n","  user_ids_timestamps = [(row['User ID'], row['Timestamp']) for _, row in filtered_behaviors_df.iterrows()]\n","\n","  return user_ids_timestamps"],"metadata":{"id":"KqYwWtmFVOLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Model: Random Recommender"],"metadata":{"id":"ltRfYlCH-sm5"}},{"cell_type":"code","source":["def single_user_recommendations_random(user_id, timestamp, k=5):\n","  # Convert input timestamp to datetime\n","  timestamp = pd.to_datetime(timestamp)\n","\n","  # Set max old article date in news_df\n","  max_old_date = timestamp - timedelta(weeks = 2)\n","\n","  # Filter news_df for articles released in the last 2 weeks before input timestamp\n","  filtered_news_df = news_df[(news_df[\"Release Date\"] < timestamp) & (news_df[\"Release Date\"] > max_old_date)]\n","\n","  # Put all article IDs in a list\n","  article_ids = filtered_news_df[\"News ID\"].tolist()\n","\n","  # Randomly select k elements\n","  final_recommended_article_ids = random.sample(article_ids, k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"V8BNJb6S-xLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recomendations_random(user_ids_timestamps, k=5):\n","  # Create empty dictionary to store recommendations\n","  user_recommendations_dict = {}\n","\n","  # Keep track of how many iterations have run\n","  counter = 0\n","\n","  # Iterate over users & timestamps\n","  for user_id, timestamp in user_ids_timestamps:\n","    # Update counter\n","    counter += 1\n","    print(counter)\n","    user_recommendations_dict[(user_id, timestamp)] = single_user_recommendations_random(user_id, timestamp, k=k)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"nr6yIpSNB7eB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model 1: Frequency & Category Recommender"],"metadata":{"id":"PiQ2oebyxco3"}},{"cell_type":"code","source":["def single_user_recomendations_frequency(user_id, timestamp, categories=None, k=5):\n","\n","    if behaviors_df[\"User ID\"].isin([user_id]).any():\n","        # User exists in the DataFrame, implement logic for recommending articles based on user history\n","        sorted_df = behaviors_df.sort_values(by = \"Timestamp\", ascending = False)\n","        user_last_interaction = sorted_df.loc[behaviors_df[\"User ID\"] == user_id].iloc[-1]\n","\n","        user_history = user_last_interaction[\"History\"].split(\" \")\n","        user_history = pd.Series(user_history).explode()\n","\n","        # Count the occurrences of each news article in user history\n","        article_counts = user_history.value_counts()\n","        # Get the top 3 categories based on the most-read articles in user history\n","        top_categories = article_counts.index.map(news_df.set_index(\"News ID\")[\"Category\"]).value_counts().index[:3]\n","\n","        behaviors_df[\"Timestamp\"] = pd.to_datetime(behaviors_df[\"Timestamp\"])\n","\n","        # Specify the datetime threshold\n","        timestamp_threshold = pd.to_datetime(timestamp)\n","\n","        # Set max old article date in df\n","        max_old_date = timestamp_threshold - timedelta(weeks = 2)\n","\n","        # Filter the \"behaviors_train\" DataFrame for timestamps greater than the threshold\n","        filtered_behaviors = behaviors_df[(behaviors_df[\"Timestamp\"] < timestamp_threshold) & (behaviors_df[\"Timestamp\"] > max_old_date)]\n","\n","        filtered_behaviors.drop(\"Timestamp\", axis=1, inplace=True)\n","\n","        articles_df = filtered_behaviors[\"History\"].str.split(\" \").explode('History')\n","        articles_df = articles_df.to_frame()\n","        articles_df['lectures'] = 1\n","\n","        articles_most_read = articles_df.groupby(\"History\").sum().sort_values(by=\"lectures\", ascending=False)\n","        articles_most_read[\"Category\"] = articles_most_read.index.map(news_df.set_index(\"News ID\")[\"Category\"].get)\n","        # Filter articles_df for the top 3 categories\n","        filtered_articles = articles_most_read[articles_most_read[\"Category\"].isin(top_categories)]\n","        # Remove articles that the user has already read\n","        filtered_articles = filtered_articles[~filtered_articles.index.isin(user_history)]\n","        # Recommend the K most popular articles\n","        recommended_articles = filtered_articles.index[:k].to_list()\n","\n","        return recommended_articles\n","\n","    else:\n","      # Convert the \"Timestamp\" column to datetime format\n","      behaviors_df[\"Timestamp\"] = pd.to_datetime(behaviors_df[\"Timestamp\"])\n","\n","      # Specify the datetime threshold\n","      timestamp_threshold = pd.to_datetime(timestamp)\n","\n","      # Set max old article date in df\n","      max_old_date = timestamp_threshold - timedelta(weeks = 2)\n","\n","      # Filter the \"behaviors_train\" DataFrame for timestamps greater than the threshold\n","      filtered_behaviors = behaviors_df[(behaviors_df[\"Timestamp\"] < timestamp_threshold) & (behaviors_df[\"Timestamp\"] > max_old_date)]\n","\n","      filtered_behaviors.drop(\"Timestamp\", axis=1, inplace=True)\n","\n","      articles_df = filtered_behaviors[\"History\"].str.split(\" \").explode('History')\n","      articles_df = articles_df.to_frame()\n","      articles_df['lectures'] = 1\n","\n","      articles_most_read = articles_df.groupby(\"History\").sum().sort_values(by=\"lectures\", ascending=False)\n","\n","      articles_most_read[\"Category\"] = articles_most_read.index.map(news_df.set_index(\"News ID\")[\"Category\"].get)\n","\n","      # Filter articles_most_read for the specified categories\n","      filtered_articles = articles_most_read[articles_most_read[\"Category\"].isin(categories)]\n","\n","      # Retrieve the top k articles\n","      final_recommended_article_ids = filtered_articles.index[:k].to_list()\n","\n","      return final_recommended_article_ids"],"metadata":{"id":"B0b1aYEJ1wCV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recomendations_frequency(user_ids_timestamps, categories=None, k=5):\n","  # Create empty dictionary to store recommendations\n","  user_recommendations_dict = {}\n","\n","  # Keep track of how many iterations have run\n","  counter = 0\n","\n","  # Iterate over users & timestamps\n","  for user_id, timestamp in user_ids_timestamps:\n","    # Update counter\n","    counter += 1\n","    print(counter)\n","    user_recommendations_dict[(user_id, timestamp)] = single_user_recomendations_frequency(user_id, timestamp, categories=categories, k=k)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"BuYK6hjK8mQb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model 2: Content Based Recommender"],"metadata":{"id":"fC5awKYWv7N1"}},{"cell_type":"markdown","source":["## Define Functions for Content Based Filtering\n","\n"],"metadata":{"id":"jaeaEujvWNCn"}},{"cell_type":"markdown","source":["### Word2Vec (Glove Model)"],"metadata":{"id":"pvbWGjvIvqHt"}},{"cell_type":"code","source":["def create_previously_read_content(user_id, timestamp):\n","  '''Inputs:\n","  previously_read_article_ids: list of article IDs previously read by a given user, provided by recommend_articles_collaborative function\n","  news_df: clean news dataframe imported from drive\n","\n","  Outputs:\n","  previously_read_content: list of words in all of the articles that were previously read by a given user\n","  '''\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # create filtered news df for articles previously read by a user\n","  previously_read_articles_df = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), ['News ID', 'Content']]\n","\n","  # create list of words containing all content words from rpeviously read articles\n","  previously_read_content = ' '.join(previously_read_articles_df['Content']).split()\n","\n","  return previously_read_content"],"metadata":{"id":"3gLlcl-owBNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_unread_content(user_id, timestamp):\n","  '''Inputs:\n","  recommended_article_ids: list of article IDs recommended by the recommend_articles_collaborative function\n","  news_df: clean news dataframe imported from drive\n","\n","  Outputs:\n","  recommended_content: dictionary with recommended article_ids as keys and list of words in article content as values\n","  '''\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","  # create filtered news df for unread articles\n","  unread_articles_df = news_df.loc[~news_df['News ID'].isin(previously_read_article_ids), ['News ID', 'Content']]\n","\n","  # Create an empty dictionary\n","  unread_content = {}\n","\n","  # Iterate over the rows of the DataFrame\n","  for row in unread_articles_df.itertuples(index=False):\n","      news_id = row[0]\n","      content = row[1]\n","\n","      # Split the content string into words\n","      words = content.split()\n","\n","      # Add the key-value pair to the dictionary\n","      unread_content[news_id] = words\n","\n","  return unread_content"],"metadata":{"id":"oOUUKO0r3qnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def single_user_recommendations_pure_content_glove(previously_read_content, unread_content, k=5):\n","    \"\"\"\n","    previously_read_content: list of content words in a user's previously read articles obtained from create_previously_read_content function\n","    unread_content: dictionary with article_ids as keys and list of words in article content as values obtained from create_unread_content function\n","    \"\"\"\n","    # Remove words not present in the GloVe model from interests & articles\n","    previously_read_content = [content for content in previously_read_content if content in glove_model]\n","    unread_content = {key: [word for word in content if word.lower() in glove_model] for key, content in unread_content.items()}\n","\n","    # Create empty list to store key-value pairs\n","    key_value_pairs = []\n","\n","    # Iterate through articles dictionary\n","    for news_id, content in unread_content.items():\n","        # Calculate cosine similarity between the list of keywords of an article and the list of user interests\n","        article_embedding = np.mean([glove_model[word] for word in content], axis=0)\n","        interest_embedding = np.mean([glove_model[word] for word in previously_read_content], axis=0)\n","        similarity_score = cosine_similarity([interest_embedding], [article_embedding])[0][0]\n","\n","        key_value_pairs.append((news_id, similarity_score))\n","\n","    # Sort the key-value pairs based on the similarity score (descending order)\n","    key_value_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Get the top n key-value pairs\n","    final_recommended_articles = key_value_pairs[:k]\n","\n","    return final_recommended_articles"],"metadata":{"id":"vFzMTJV2_KKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content_glove(user_ids_timestamps, articles_k=5):\n","  # Create empty dictionary to store recommendations\n","  user_recommendations_dict = {}\n","\n","  # Initiate counter\n","  counter = 0\n","\n","  # Iterate over users & timestamps\n","  for user_id, timestamp in user_ids_timestamps:\n","    # Update counter\n","    counter += 1\n","    print(counter)\n","    # create previously read content list\n","    previously_read_content = create_previously_read_content(user_id, timestamp)\n","    # create unread content dictionary\n","    unread_content = create_unread_content(user_id, timestamp)\n","    # find final similar articles\n","    final_recommended_ids = single_user_recommendations_pure_content_glove(previously_read_content, unread_content, k=articles_k)\n","    user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","  return user_recommendations_dict"],"metadata":{"id":"tG2fK8tExckR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Embeddings"],"metadata":{"id":"QzcrwW7RYypA"}},{"cell_type":"code","source":["def single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=5):\n","   # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_news_vector = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), 'Average Vector'].mean()\n","\n","  # Filter news_df to exlcude articles in user history\n","  filtered_news_df = news_df.loc[~news_df['News ID'].isin(previously_read_article_ids)]\n","\n","  # Convert input timestamp to date time\n","  timestamp = pd.to_datetime(timestamp)\n","\n","  # # Filter news_df to exlcude any articles released after date of interaction\n","  filtered_news_df = filtered_news_df[filtered_news_df['Release Date'] <= timestamp]\n","\n","  # Compute cosine similarity between average_news_vector and each unread news article\n","  filtered_news_df['Similarity'] = filtered_news_df['Average Vector'].apply(lambda x: cosine_similarity([average_news_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  filtered_news_df = filtered_news_df.sort_values(by='Similarity', ascending=False)\n","\n","  # Select top k articles and return as a list of tuples\n","  final_recommended_articles = [(news_id, similarity) for news_id, similarity in zip(filtered_news_df.head(articles_k)['News ID'], filtered_news_df.head(articles_k)['Similarity'])]\n","\n","  return final_recommended_articles\n"],"metadata":{"id":"sPpuEm2hZ81n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content_embeddings(user_ids_timestamps, articles_k=5):\n","  '''Inputs:\n","  user_ids_timestaps: tuple with user_id & timestamp\n","  '''\n","  # create an empty dictionary to populate with recommendations\n","  user_recommendations_dict = {}\n","\n","  # initiate counter to track progress\n","  counter = 0\n","\n","  for user_id, timestamp in user_ids_timestamps:\n","    # run function for single users\n","    final_recommended_ids = single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=articles_k)\n","\n","    # create dictionary for final recommendations\n","    user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","    # increase counter\n","    counter += 1\n","    print(counter)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"xSjesU0gc5a-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TFIDF"],"metadata":{"id":"-E6zD-rnZRK5"}},{"cell_type":"code","source":["def create_tfidf_features(news_df):\n","  # Create the TF-IDF vectorizer with preprocessing\n","  tfidf = TfidfVectorizer(strip_accents=None,\n","                          lowercase=True,\n","                          tokenizer=word_tokenize,\n","                          use_idf=True,\n","                          norm='l2',\n","                          smooth_idf=True,\n","                          stop_words='english',\n","                          max_df=0.5,\n","                          sublinear_tf=True)\n","\n","  # Fit and transform the combined column\n","  features = tfidf.fit_transform(news_df['Content'])\n","\n","  return features"],"metadata":{"id":"yeNbP3PKZSed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=5):\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get the indices of the relevant news in the features matrix (removing those read already)\n","  previously_read_indices = news_df[news_df['News ID'].isin(previously_read_article_ids)].index.tolist()\n","\n","  # # Get the indices of news articles read after the input timestamp (to exclude them in the next step)\n","  timestamp = pd.to_datetime(timestamp)\n","\n","  all_indices = list(range(features.shape[0]))\n","  future_article_indices = news_df[news_df['Release Date'] > timestamp].index.tolist()\n","  not_previously_read_indices = [idx for idx in all_indices if idx not in previously_read_indices and idx not in future_article_indices]\n","\n","  # Aggregate the feature vectors of the read articles\n","  user_profile = np.asarray(features[previously_read_indices].sum(axis=0)/len(previously_read_indices))\n","\n","  # Calculate the similarity scores between the user profile and other articles\n","  similarity_scores = cosine_similarity(user_profile.reshape(1, -1), features[not_previously_read_indices]).flatten()\n","\n","  # Find the indices of the top 5 recommendations\n","  top_indices = similarity_scores.argsort()[-articles_k:][::-1]\n","\n","  # Get the top recommended news articles as a list of tuples\n","  final_recommended_article_ids = [(news_id, similarity) for news_id, similarity in\n","                                zip(list(news_df.iloc[np.array(not_previously_read_indices)[top_indices].tolist(),]['News ID']),\n","                                    similarity_scores[top_indices])]\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"McxSOkg9aNrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def single_user_recommendations_pure_content_tfidf(user_id, timestamp, articles_k=5):\n","  # Create features\n","  features = create_tfidf_features(news_df)\n","\n","  #Run recommendations function\n","  final_recommended_article_ids = recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"YsnnInazun-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content_tfidf(user_ids_timestamps, articles_k=5):\n","  # Create features\n","  features = create_tfidf_features(news_df)\n","\n","  # create an empty dictionary to populate with recommendations\n","  user_recommendations_dict = {}\n","\n","  # initiate counter to track progress\n","  counter = 0\n","\n","  for user_id, timestamp in user_ids_timestamps:\n","    # run function for single users\n","    final_recommended_ids = recommendations_pure_content_tfidf(user_id, timestamp, features, articles_k=articles_k)\n","\n","    # create dictionary for final recommendations\n","    user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","    # increase counter\n","    counter += 1\n","    print(counter)\n","\n","  return user_recommendations_dict\n","\n"],"metadata":{"id":"vVJxoMeQqNDl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Functions that consolidate Pipeline"],"metadata":{"id":"xXy5hx8Wv5ey"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"R6cwLED9Md8h"}},{"cell_type":"code","source":["def single_user_recommendations_pure_content(user_id, timestamp, method='embeddings', articles_k=10):\n","  if method == 'word2vec_glove':\n","      # create previously read content list\n","    previously_read_content = create_previously_read_content(user_id, timestamp)\n","    # create unread content dictionary\n","    unread_content = create_unread_content(user_id, timestamp)\n","    final_recommended_article_ids = single_user_recommendations_pure_content_glove(previously_read_content, unread_content, k=articles_k)\n","\n","  elif method == 'embeddings':\n","    final_recommended_article_ids = single_user_recommendations_pure_content_embeddings(user_id, timestamp, articles_k=articles_k)\n","\n","  elif method == 'tfidf':\n","    final_recommended_article_ids = single_user_recommendations_pure_content_tfidf(user_id, timestamp, articles_k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"cOwxEL0tv69D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multiple Users"],"metadata":{"id":"yaWlJxE3MgK-"}},{"cell_type":"code","source":["def multiple_user_recommendations_pure_content(user_ids_timestamps, method='embeddings', articles_k=10):\n","  user_recommendations_dict = {}\n","\n","  if method == 'word2vec_glove':\n","    user_recommendations_dict = multiple_user_recommendations_pure_content_glove(user_ids_timestamps, articles_k=articles_k)\n","  elif method == 'embeddings':\n","    user_recommendations_dict = multiple_user_recommendations_pure_content_embeddings(user_ids_timestamps, articles_k=articles_k)\n","  elif method == 'tfidf':\n","    user_recommendations_dict = multiple_user_recommendations_pure_content_tfidf(user_ids_timestamps, articles_k=articles_k)\n","\n","  return user_recommendations_dict"],"metadata":{"id":"4TpBaA7Rw1Aw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model 3: Collaborative & Content Based Recommender"],"metadata":{"id":"hxB0QzDoiQKo"}},{"cell_type":"markdown","metadata":{"id":"4CKAv7qduqNt"},"source":["## Define Functions for User to User Collaborative Filtering"]},{"cell_type":"code","source":["def fetch_similar_users(user_id, timestamp, k=5):\n","  # Get IDs in user's history\n","  presiouvly_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_user_vector = news_df.loc[news_df['News ID'].isin(presiouvly_read_article_ids), 'Average Vector'].mean()\n","\n","  # Create a copy of behaviours_df\n","  user_similarity_df = behaviors_df.copy()\n","\n","  # Removes users without history & impressions\n","  user_similarity_df = user_similarity_df.dropna()\n","\n","  # Filter out input user from user_similarity_df\n","  user_similarity_df = user_similarity_df.loc[user_similarity_df['User ID'] != user_id]\n","\n","  # Drop duplicate users\n","  user_similarity_df = user_similarity_df.drop_duplicates(subset=['User ID', 'History & Impressions'])\n","\n","  # Compute cosine similarity between average_news_vector and each user\n","  user_similarity_df['Similarity'] = user_similarity_df['Average Vector'].apply(lambda x: cosine_similarity([average_user_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  user_similarity_df = user_similarity_df.sort_values(by='Similarity', ascending=False).head(k)\n","\n","  # Get similar users\n","  similar_users_timestamps = [(row['User ID'], row['Timestamp']) for _, row in user_similarity_df.iterrows()]\n","\n","  return similar_users_timestamps"],"metadata":{"id":"dNNlt9zUzvTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps):\n","  # Filter behaviors df for similar users & timestamps\n","  similar_users_df = behaviors_df[behaviors_df[['User ID', 'Timestamp']].apply(tuple, axis=1).isin(similar_users_timestamps)]\n","\n","  # Initialize list to store relevant article IDs\n","  recommended_article_ids = []\n","\n","  # Iterate over the rows of the DataFrame\n","  for index, row in similar_users_df.iterrows():\n","    # Split the text into words and add them to the word_list\n","    recommended_article_ids.extend(row['History & Impressions'].split())\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Remove any already read articles from the recommended articles\n","  recommended_article_ids = list(set([id for id in recommended_article_ids if id not in previously_read_article_ids]))\n","\n","  return recommended_article_ids"],"metadata":{"id":"LfVNjJrm3Bmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpCRATmLuqNu"},"source":["## Define Functions for Content Based Filtering - Post Collaborative Filtering"]},{"cell_type":"markdown","source":["### Word2Vec"],"metadata":{"id":"2CTye4a4O431"}},{"cell_type":"code","source":["def create_previously_read_content(user_id, timestamp):\n","  '''Inputs:\n","  previously_read_article_ids: list of article IDs previously read by a given user, provided by recommend_articles_collaborative function\n","  news_df: clean news dataframe imported from drive\n","\n","  Outputs:\n","  previously_read_content: list of words in all of the articles that were previously read by a given user\n","  '''\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # create filtered news df for articles previously read by a user\n","  previously_read_articles_df = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), ['News ID', 'Content']]\n","\n","  # create list of words containing all content words from rpeviously read articles\n","  previously_read_content = ' '.join(previously_read_articles_df['Content']).split()\n","\n","  return previously_read_content"],"metadata":{"id":"Gue3O2a-OVw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_recommended_content(recommended_article_ids):\n","    '''Inputs:\n","    recommended_article_ids: list of article IDs recommended by the recommend_articles_collaborative function\n","    news_df: clean news dataframe imported from drive\n","\n","    Outputs:\n","    recommended_content: dictionary with recommended article_ids as keys and list of words in article content as values\n","    '''\n","    # create filtered news df for recommended articles\n","    recommended_articles_df = news_df.loc[news_df['News ID'].isin(recommended_article_ids), ['News ID', 'Content']]\n","\n","    # Create an empty dictionary\n","    recommended_content = {}\n","\n","    # Iterate over the rows of the DataFrame\n","    for row in recommended_articles_df.itertuples(index=False):\n","        news_id = row[0]\n","        content = row[1]\n","\n","        # Split the content string into words\n","        words = content.split()\n","\n","        # Add the key-value pair to the dictionary\n","        recommended_content[news_id] = words\n","\n","    return recommended_content"],"metadata":{"id":"ffN4kMLqQQZy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Word2vec (Google Model)"],"metadata":{"id":"PQ_bbFfLpsQH"}},{"cell_type":"code","source":["# function to process inputs and identify most relevant article (based on cosine similarity)\n","def recommend_articles_content_google(previously_read_content, recommended_content, k=5):\n","    '''\n","    previously_read_content: list of content words in a user's previously read articles obtained from create_previously_read_content function\n","    recommended_content: dictionary with article_ids as keys and list of words in article content as values obtained from create_recommended_content function\n","    '''\n","    # Remove words not used in model training from interests & articles\n","    previously_read_content = [content for content in previously_read_content if content in list(google_model.key_to_index.keys())]\n","    recommended_content = {key: [word for word in content if word.lower() in google_model.key_to_index] for key, content in recommended_content.items()}\n","\n","    # Create empty list to store key-value pairs\n","    key_value_pairs = []\n","\n","    # iterate through articles dictionary\n","    for news_id, content in recommended_content.items():\n","        # calculate cosine similarity between the list of keywords of an article and the list of user interests\n","        similarity_score = google_model.n_similarity(previously_read_content, content)\n","\n","        key_value_pairs.append((news_id, similarity_score))\n","\n","    # Sort the key-value pairs based on the similarity score (descending order)\n","    key_value_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Get the top n key-value pairs\n","    final_recommended_article_ids = key_value_pairs[:k]\n","\n","    return final_recommended_article_ids"],"metadata":{"id":"PSxqGiURQ4-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Word2vec (Glove Model)"],"metadata":{"id":"xFuBq5eVpvG6"}},{"cell_type":"code","source":["# function to process inputs and identify most relevant article (based on cosine similarity)\n","def recommend_articles_content_glove(previously_read_content, recommended_content, k=5):\n","    \"\"\"\n","    previously_read_content: list of content words in a user's previously read articles obtained from create_previously_read_content function\n","    recommended_content: dictionary with article_ids as keys and list of words in article content as values obtained from create_recommended_content function\n","    \"\"\"\n","    # Remove words not present in the GloVe model from interests & articles\n","    previously_read_content = [content for content in previously_read_content if content in glove_model]\n","    recommended_content = {key: [word for word in content if word.lower() in glove_model] for key, content in recommended_content.items()}\n","\n","    # Create empty list to store key-value pairs\n","    key_value_pairs = []\n","\n","    # Iterate through articles dictionary\n","    for news_id, content in recommended_content.items():\n","        # Calculate cosine similarity between the list of keywords of an article and the list of user interests\n","        article_embedding = np.mean([glove_model[word] for word in content], axis=0)\n","        interest_embedding = np.mean([glove_model[word] for word in previously_read_content], axis=0)\n","        similarity_score = cosine_similarity([interest_embedding], [article_embedding])[0][0]\n","\n","        key_value_pairs.append((news_id, similarity_score))\n","\n","    # Sort the key-value pairs based on the similarity score (descending order)\n","    key_value_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Get the top n key-value pairs\n","    final_recommended_article_ids = key_value_pairs[:k]\n","\n","    return final_recommended_article_ids"],"metadata":{"id":"rGMDXhTxpwRu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Embeddings"],"metadata":{"id":"iGe6OANaSNJn"}},{"cell_type":"code","source":["def get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=5):\n","\n","  # create filtered_news_df based on recommended articles from collaborative based filtering\n","  filtered_news_df = news_df.loc[news_df['News ID'].isin(recommended_article_ids)]\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","  )\n","\n","  # Get average vector of user's history news IDs\n","  average_news_vector = news_df.loc[news_df['News ID'].isin(previously_read_article_ids), 'Average Vector'].mean()\n","\n","  # Filter news_df to exlcude articles in user history\n","  filtered_news_df = filtered_news_df.loc[~filtered_news_df['News ID'].isin(previously_read_article_ids)]\n","\n","  # Compute cosine similarity between average_news_vector and each unread news article\n","  filtered_news_df['Similarity'] = filtered_news_df['Average Vector'].apply(lambda x: cosine_similarity([average_news_vector], [x])[0][0])\n","\n","  # Sort dataframe in descending order\n","  filtered_news_df = filtered_news_df.sort_values(by='Similarity', ascending=False)\n","\n","  # Select top k articles and return as a list of tuples\n","  top_k_recommended_article_ids = [(news_id, similarity) for news_id, similarity in zip(filtered_news_df.head(k)['News ID'], filtered_news_df.head(k)['Similarity'])]\n","\n","  return top_k_recommended_article_ids"],"metadata":{"id":"CaBGvL5pKTk4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFRePjiQXuvB"},"source":["### TFIDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykRmF59IYNg1"},"outputs":[],"source":["def create_tfidf_features(news_df):\n","  # Create the TF-IDF vectorizer with preprocessing\n","  tfidf = TfidfVectorizer(strip_accents=None,\n","                          lowercase=True,\n","                          tokenizer=word_tokenize,\n","                          use_idf=True,\n","                          norm='l2',\n","                          smooth_idf=True,\n","                          stop_words='english',\n","                          max_df=0.5,\n","                          sublinear_tf=True)\n","\n","  # Fit and transform the combined column\n","  features = tfidf.fit_transform(news_df['Content'])\n","\n","  return features"]},{"cell_type":"code","source":["def recommend_articles_content_tfidf(user_id, timestamp, recommended_articles_ids, features, k=5):\n","\n","  # Get IDs in user's history\n","  previously_read_article_ids = (\n","      list(\n","          behaviors_df.loc[\n","              ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n","              'History'\n","          ].str.split()\n","      )[0]\n","    )\n","\n","  # Get the indices of the relevant news in the features matrix (removing those read already)\n","  news_indices = news_df[news_df['News ID'].isin(previously_read_article_ids)].index.tolist()\n","\n","  # Get the indices of the recommended news in the features matrix\n","  recomended_articles_indices = news_df[news_df['News ID'].isin(recommended_articles_ids)].index.tolist()\n","\n","  all_indices = list(range(features.shape[0]))\n","  not_news_indices = [idx for idx in all_indices if idx in recomended_articles_indices]\n","\n","  # Aggregate the feature vectors of the read articles\n","  user_profile = np.asarray(features[news_indices].sum(axis=0)/len(news_indices))\n","\n","  # Calculate the similarity scores between the user profile and other articles\n","  similarity_scores = cosine_similarity(user_profile.reshape(1, -1), features[not_news_indices]).flatten()\n","\n","  # Find the indices of the top 5 recommendations\n","  top_indices = similarity_scores.argsort()[-k:][::-1]\n","\n","  # Get the top recommended news articles as a list of tuples\n","  final_recommended_article_ids = [(news_id, similarity) for news_id, similarity in\n","                                zip(list(news_df.iloc[np.array(not_news_indices)[top_indices].tolist(),]['News ID']),\n","                                    similarity_scores[top_indices])]\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"WgTWlaorMhVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9v6AABOuqNv"},"source":["## Define Functions that consolidate Pipeline"]},{"cell_type":"markdown","metadata":{"id":"cARuG1m6uqNv"},"source":["### Single User"]},{"cell_type":"code","source":["def single_user_recommendations_combined(user_id, timestamp, method='word2vec', similar_user_k=5, articles_k=5):\n","\n","  # Get user ID & time stamp of similar user interactions\n","  similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n","\n","  # Get article IDs read by similar user interactions\n","  recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n","\n","  # Apply embeddings or TFIDF methodology\n","\n","  if method=='word2vec_google':\n","    # create previously read content list\n","    previously_read_content = create_previously_read_content(user_id, timestamp)\n","    # create recommended content dictionary\n","    recommended_content = create_recommended_content(recommended_article_ids)\n","    # find final similar articles\n","    final_recommended_article_ids = recommend_articles_content_google(previously_read_content, recommended_content, k=articles_k)\n","\n","  if method=='word2vec_glove':\n","    # create previously read content list\n","    previously_read_content = create_previously_read_content(user_id, timestamp)\n","    # create recommended content dictionary\n","    recommended_content = create_recommended_content(recommended_article_ids)\n","    # find final similar articles\n","    final_recommended_article_ids = recommend_articles_content_glove(previously_read_content, recommended_content, k=5)\n","\n","  elif method == 'embeddings':\n","    # find final similar articles\n","    final_recommended_article_ids = get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=articles_k)\n","\n","  elif method == 'tfidf':\n","    # create features\n","    features = create_tfidf_features(news_df)\n","    # find final similar articles\n","    final_recommended_article_ids = recommend_articles_content_tfidf(user_id, timestamp, recommended_article_ids, features, k=articles_k)\n","\n","  return final_recommended_article_ids"],"metadata":{"id":"1kFAbfvqJMEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLdgvwoQuqNv"},"source":["### Multiple Users"]},{"cell_type":"code","source":["def multiple_user_recommendations_combined(user_ids_timestamps, method='tfidf', similar_user_k=5, articles_k=5):\n","\n","  # Create empty dictionary to store recommendations\n","  user_recommendations_dict = {}\n","\n","  # If method is tfidf create features\n","  if method == 'tfidf':\n","    features = create_tfidf_features(news_df)\n","\n","  # Keep track of how many iterations have run\n","  counter = 0\n","\n","  # Iterate over users & timestamps\n","  for user_id, timestamp in user_ids_timestamps:\n","    # Update counter\n","    counter += 1\n","    print(counter)\n","\n","    # Get user ID & time stamp of similar user interactions\n","    similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n","\n","    # Get article IDs read by similar user interactions\n","    recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n","\n","    if method=='word2vec_google':\n","      # create previously read content list\n","      previously_read_content = create_previously_read_content(user_id, timestamp)\n","      # create recommended content dictionary\n","      recommended_content = create_recommended_content(recommended_article_ids)\n","      # find final similar articles\n","      final_recommended_ids = recommend_articles_content_google(previously_read_content, recommended_content, k=articles_k)\n","      user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","    if method=='word2vec_glove':\n","      # create previously read content list\n","      previously_read_content = create_previously_read_content(user_id, timestamp)\n","      # create recommended content dictionary\n","      recommended_content = create_recommended_content(recommended_article_ids)\n","      # find final similar articles\n","      final_recommended_ids = recommend_articles_content_glove(previously_read_content, recommended_content, k=5)\n","      user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","    # Apply embeddings or TFIDF methodology\n","    elif method == 'embeddings':\n","      final_recommended_ids = get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=articles_k)\n","      user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","    elif method == 'tfidf':\n","      final_recommended_ids = recommend_articles_content_tfidf(user_id, timestamp, recommended_article_ids, features, k=articles_k)\n","      user_recommendations_dict[(user_id, timestamp)] = final_recommended_ids\n","\n","  return user_recommendations_dict"],"metadata":{"id":"3lHdtvGIG_pY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make Predictions"],"metadata":{"id":"Y90WsWTe6ADs"}},{"cell_type":"code","source":["# Select number of users to evaluate on\n","number_test_users = 5\n","# Select a subset of users of size k to test on\n","user_ids_timestamps = select_user_ids_timestamps(minimum_history=15, minimum_impressions=8, k=number_test_users)"],"metadata":{"id":"C-GlMCPb7B4N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Baseline Model: Random Recommender"],"metadata":{"id":"1lPFnpmDDJga"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"5kxjOX7WDPUz"}},{"cell_type":"code","source":["# Run model\n","final_recommended_ids = single_user_recommendations_random(\"U8500\", \"2019-11-13 15:27:40\", k=10)\n","final_recommended_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rpm24QYCDMdu","executionInfo":{"status":"ok","timestamp":1688462269950,"user_tz":-120,"elapsed":18,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"55c5e929-6339-40d4-9296-d1a6a71907f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['N25120',\n"," 'N15540',\n"," 'N59374',\n"," 'N47392',\n"," 'N913',\n"," 'N9244',\n"," 'N22818',\n"," 'N43368',\n"," 'N12397',\n"," 'N40421']"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["### Multi User"],"metadata":{"id":"9HgOEDrCDQwe"}},{"cell_type":"code","source":["# Run model\n","final_recommended_ids_multiple = multiple_user_recomendations_random(user_ids_timestamps, k=10)\n","final_recommended_ids_multiple"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b28BmAbjDS2X","executionInfo":{"status":"ok","timestamp":1688462275689,"user_tz":-120,"elapsed":5755,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"7f963ccf-9474-4233-bd6c-90ec738d2f73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n"]},{"output_type":"execute_result","data":{"text/plain":["{'U11241': ['N31801',\n","  'N16715',\n","  'N4607',\n","  'N46392',\n","  'N59704',\n","  'N619',\n","  'N32004',\n","  'N1150',\n","  'N13138',\n","  'N16233'],\n"," 'U46273': ['N306',\n","  'N31801',\n","  'N45794',\n","  'N16715',\n","  'N46392',\n","  'N59704',\n","  'N619',\n","  'N1150',\n","  'N28088',\n","  'N16233'],\n"," 'U50429': ['N31801',\n","  'N45794',\n","  'N43142',\n","  'N16715',\n","  'N51706',\n","  'N54827',\n","  'N46392',\n","  'N59704',\n","  'N41375',\n","  'N27448'],\n"," 'U85170': ['N42620',\n","  'N871',\n","  'N29177',\n","  'N55189',\n","  'N18870',\n","  'N33276',\n","  'N4020',\n","  'N60702',\n","  'N35022',\n","  'N35671'],\n"," 'U77421': ['N42620',\n","  'N31801',\n","  'N45794',\n","  'N16715',\n","  'N46392',\n","  'N54827',\n","  'N59704',\n","  'N18870',\n","  'N55743',\n","  'N41375']}"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# # Convert to exportable format\n","# final_recommended_ids_multiple = {json.dumps(key): value for key, value in final_recommended_ids_multiple.items()}\n","\n","# # Export results\n","# with open(f'/content/drive/MyDrive/Group_19/01.Dataset/Predictions/predictions_baseline_model_{number_test_users}_users.json', 'w') as json_file:\n","#     json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"AMkTCK-bErKM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 1: Frequency & Category Recommender"],"metadata":{"id":"F1dxcZF18KnL"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"q1sB6IY78RD8"}},{"cell_type":"code","source":["# Run model\n","final_recommended_ids = single_user_recomendations_frequency(\"U8500\", \"2019-11-13 15:27:40\", [\"news\", \"weather\", \"lifestyle\"], k=10)\n","final_recommended_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d5RNicc8Lyw","executionInfo":{"status":"ok","timestamp":1688462279190,"user_tz":-120,"elapsed":3518,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"3a14169c-33f1-4217-f3a0-c1c96f2d5c8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['N42620',\n"," 'N31801',\n"," 'N45794',\n"," 'N16715',\n"," 'N46392',\n"," 'N54827',\n"," 'N59704',\n"," 'N18870',\n"," 'N55743',\n"," 'N32004']"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["### Multi User"],"metadata":{"id":"WW0Wg12Z9oU_"}},{"cell_type":"code","source":["# Run model\n","final_recommended_ids_multiple = multiple_user_recomendations_frequency(user_ids_timestamps, categories=None, k=10)\n","final_recommended_ids_multiple"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGzoA9i59ne5","executionInfo":{"status":"ok","timestamp":1688462288764,"user_tz":-120,"elapsed":9586,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"99a790bd-e1e4-435d-81a0-7607179b9edc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n"]},{"output_type":"execute_result","data":{"text/plain":["{'U11241': ['N31801',\n","  'N16715',\n","  'N4607',\n","  'N46392',\n","  'N59704',\n","  'N619',\n","  'N32004',\n","  'N1150',\n","  'N13138',\n","  'N16233'],\n"," 'U46273': ['N306',\n","  'N31801',\n","  'N45794',\n","  'N16715',\n","  'N46392',\n","  'N59704',\n","  'N619',\n","  'N1150',\n","  'N28088',\n","  'N16233'],\n"," 'U50429': ['N31801',\n","  'N45794',\n","  'N43142',\n","  'N16715',\n","  'N51706',\n","  'N54827',\n","  'N46392',\n","  'N59704',\n","  'N41375',\n","  'N27448'],\n"," 'U85170': ['N42620',\n","  'N871',\n","  'N29177',\n","  'N55189',\n","  'N18870',\n","  'N33276',\n","  'N4020',\n","  'N60702',\n","  'N35022',\n","  'N35671'],\n"," 'U77421': ['N42620',\n","  'N31801',\n","  'N45794',\n","  'N16715',\n","  'N46392',\n","  'N54827',\n","  'N59704',\n","  'N18870',\n","  'N55743',\n","  'N41375']}"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# # Convert to exportable format\n","# final_recommended_ids_multiple = {json.dumps(key): value for key, value in final_recommended_ids_multiple.items()}\n","\n","# # Export results\n","# with open(f'/content/drive/MyDrive/Group_19/01.Dataset/Predictions/predictions_model1_{number_test_users}_users.json', 'w') as json_file:\n","#     json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"QIibBTu99x_q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 2: Content Based Recommender\n","Methods:\n","- tfidf\n","- embeddings\n","- word2vec_glove\n","\n","Note that tfidf is the least computationally expensive and word2vec_glove is the most computationally expensive"],"metadata":{"id":"Ydtwn2pp6DWW"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"VMkMJECd6HDm"}},{"cell_type":"code","source":["# Define method to use:\n","method = 'tfidf'\n","\n","# Run model\n","final_recommended_ids = single_user_recommendations_pure_content(user_id='U13740', timestamp='2019-11-13 15:27:40', method=method, articles_k=10)\n","final_recommended_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdHiBxax6CPK","executionInfo":{"status":"ok","timestamp":1688462304195,"user_tz":-120,"elapsed":15443,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"24ae3435-5045-468f-e21c-d3615e0fc011"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('N59426', 0.2389679796572606),\n"," ('N34069', 0.2357753342049917),\n"," ('N61980', 0.212007250112408),\n"," ('N59336', 0.21097985822568008),\n"," ('N628', 0.20479455592138335),\n"," ('N19522', 0.20223114224583788),\n"," ('N28476', 0.19642963415927053),\n"," ('N54172', 0.19642963415927053),\n"," ('N5035', 0.1951850328229925),\n"," ('N9080', 0.19028199938135584)]"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### Multi User"],"metadata":{"id":"r8BIJdfn6JfK"}},{"cell_type":"code","source":["# Define method to use\n","method = 'tfidf'\n","\n","# Run model\n","final_recommended_ids_multiple = multiple_user_recommendations_pure_content(user_ids_timestamps, method=method, articles_k=10)\n","final_recommended_ids_multiple"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O12gYOOM6KpA","executionInfo":{"status":"ok","timestamp":1688462323700,"user_tz":-120,"elapsed":19513,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"57bb8ba4-3b82-43fa-cf9a-dd47b1cecef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n"]},{"output_type":"execute_result","data":{"text/plain":["{('U11241', '2019-11-13 06:56:56'): [('N3736', 0.1832456463717506),\n","  ('N11196', 0.18276597206082024),\n","  ('N44506', 0.16504580014337983),\n","  ('N39235', 0.1599936156264622),\n","  ('N20930', 0.15899050062827763),\n","  ('N58637', 0.15798458863277243),\n","  ('N62353', 0.1576325960752563),\n","  ('N26123', 0.15709720017976284),\n","  ('N18741', 0.15665689684980832),\n","  ('N39380', 0.1559073751709623)],\n"," ('U46273', '2019-11-13 09:56:13'): [('N27230', 0.35617495130014276),\n","  ('N22066', 0.34850869259284145),\n","  ('N11368', 0.34809167108044603),\n","  ('N51998', 0.34799238971405944),\n","  ('N41462', 0.3449104729664711),\n","  ('N10597', 0.33811927004359327),\n","  ('N51278', 0.3265380619397497),\n","  ('N45385', 0.326427077471173),\n","  ('N49867', 0.32053660115859645),\n","  ('N5844', 0.31248449107864407)],\n"," ('U50429', '2019-11-12 19:51:39'): [('N53960', 0.3005274877075451),\n","  ('N48134', 0.29561359903961365),\n","  ('N14031', 0.29186127864644795),\n","  ('N5580', 0.2833622708675733),\n","  ('N15443', 0.28190545375493825),\n","  ('N13105', 0.27924217206349183),\n","  ('N21702', 0.2788354237256619),\n","  ('N39661', 0.2788354237256619),\n","  ('N21697', 0.2786691361812805),\n","  ('N35948', 0.2775145835806071)],\n"," ('U85170', '2019-11-13 10:24:40'): [('N1410', 0.2593793637138285),\n","  ('N63577', 0.25596466676784924),\n","  ('N7355', 0.24877904709170579),\n","  ('N35827', 0.24793521254066703),\n","  ('N40257', 0.2325902589695652),\n","  ('N50007', 0.22970252856626433),\n","  ('N18001', 0.2285198868188565),\n","  ('N21695', 0.2284983077021219),\n","  ('N14200', 0.2195057445664782),\n","  ('N1798', 0.2183665963307004)],\n"," ('U77421', '2019-11-14 08:38:07'): [('N30755', 0.17320972345157956),\n","  ('N3647', 0.16653474465708917),\n","  ('N26221', 0.15514951313323203),\n","  ('N15813', 0.15514951313323203),\n","  ('N32889', 0.1442235781471287),\n","  ('N48596', 0.14420808110207034),\n","  ('N8386', 0.1433287906389993),\n","  ('N3449', 0.14066383323870313),\n","  ('N54003', 0.13267234689038054),\n","  ('N12945', 0.1299890415561554)]}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# # Convert to exportable format\n","# final_recommended_ids_multiple = {json.dumps(key): value for key, value in final_recommended_ids_multiple.items()}\n","\n","# # Export results\n","# with open(f'/content/drive/MyDrive/Group_19/01.Dataset/Predictions/predictions_model2_{method}_{number_test_users}_users.json', 'w') as json_file:\n","#     json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"5N3CYKg_6frD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 3: Collaborative & Content Based Recommender\n","Methods:\n","- tfidf\n","- embeddings\n","- word2vec_glove\n","- word2vec_google\n","\n","Note that tfidf is the least computationally expensive and word2vec_google is the most computationally expensive\n","\n"],"metadata":{"id":"4sfVXusR64VY"}},{"cell_type":"markdown","source":["### Single User"],"metadata":{"id":"HS4vMEJu6w9H"}},{"cell_type":"code","source":["# Define method to use\n","method = 'tfidf'\n","\n","# Run model\n","final_recommended_ids = single_user_recommendations_combined('U13740', '2019-11-13 15:27:40', method=method, similar_user_k=5, articles_k=10)\n","final_recommended_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckfXlCiy63ZV","executionInfo":{"status":"ok","timestamp":1688462385714,"user_tz":-120,"elapsed":62024,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"3be64da7-6cd5-4c43-8740-7a9f71cedee0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('N62646', 0.13573152549658354),\n"," ('N28678', 0.1302081059461651),\n"," ('N4378', 0.11881006371425713),\n"," ('N23571', 0.07628988282376056),\n"," ('N33998', 0.07600726928910853),\n"," ('N17799', 0.07299124648972674),\n"," ('N36150', 0.06675080281851244),\n"," ('N21547', 0.06073790473438189),\n"," ('N2203', 0.05876959033254194),\n"," ('N41777', 0.0586725991082965)]"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["### Multi User"],"metadata":{"id":"D0RVFPaD6xPU"}},{"cell_type":"code","source":["# Define method to use\n","method = 'tfidf'\n","\n","# Run model\n","final_recommended_ids_multiple = multiple_user_recommendations_combined(user_ids_timestamps, method=method, similar_user_k=5, articles_k=10)\n","final_recommended_ids_multiple"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuSpeBiu6wDI","executionInfo":{"status":"ok","timestamp":1688462627171,"user_tz":-120,"elapsed":241475,"user":{"displayName":"Beatriz Leitao","userId":"00922298846450358063"}},"outputId":"3ada7974-0461-4ee4-aea5-2157f8422e0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n"]},{"output_type":"execute_result","data":{"text/plain":["{('U11241', '2019-11-13 06:56:56'): [('N39235', 0.1599936156264622),\n","  ('N44021', 0.1268015529185742),\n","  ('N38998', 0.12598292144696782),\n","  ('N4607', 0.10506280986184377),\n","  ('N19620', 0.10187923754169145),\n","  ('N11323', 0.09987245918044703),\n","  ('N18893', 0.09938526931447786),\n","  ('N63302', 0.09807044375344426),\n","  ('N21448', 0.09716616835629371),\n","  ('N24591', 0.09613721468410572)],\n"," ('U46273', '2019-11-13 09:56:13'): [('N39960', 0.2910661724553247),\n","  ('N46128', 0.27630906269721384),\n","  ('N831', 0.2722299425189194),\n","  ('N59704', 0.25707938073856795),\n","  ('N44676', 0.2406771078474222),\n","  ('N30241', 0.20842701709171604),\n","  ('N59173', 0.19972123729520147),\n","  ('N61943', 0.1970088343186164),\n","  ('N25898', 0.18870565180244828),\n","  ('N46392', 0.18640181345992035)],\n"," ('U50429', '2019-11-12 19:51:39'): [('N33131', 0.25626322115709976),\n","  ('N6951', 0.24859033240718564),\n","  ('N12948', 0.21530844830944126),\n","  ('N26683', 0.20004414409516394),\n","  ('N51058', 0.19787473995666338),\n","  ('N53428', 0.1904062508245708),\n","  ('N31126', 0.18197344687012948),\n","  ('N39333', 0.17956698979433722),\n","  ('N52413', 0.1744084882215184),\n","  ('N2618', 0.17027689767772006)],\n"," ('U85170', '2019-11-13 10:24:40'): [('N15634', 0.1903555415434707),\n","  ('N8953', 0.18683716372632736),\n","  ('N47698', 0.15791130445374937),\n","  ('N5791', 0.15750817553744462),\n","  ('N40207', 0.15661801758969013),\n","  ('N31802', 0.15560772003044823),\n","  ('N16770', 0.15528894395976495),\n","  ('N32924', 0.1496225294701644),\n","  ('N36557', 0.1459379367667884),\n","  ('N58860', 0.1446095040695352)],\n"," ('U77421', '2019-11-14 08:38:07'): [('N19048', 0.09030591464311268),\n","  ('N49262', 0.07630166774440691),\n","  ('N64661', 0.074631265025664),\n","  ('N47412', 0.07418501329896325),\n","  ('N30589', 0.07363527755231139),\n","  ('N1150', 0.06961662187734799),\n","  ('N22952', 0.06785901384857672),\n","  ('N51821', 0.06487112029005389),\n","  ('N40837', 0.06372377630265862),\n","  ('N13304', 0.06287026484057665)]}"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# # Convert to exportable format\n","# final_recommended_ids_multiple = {json.dumps(key): value for key, value in final_recommended_ids_multiple.items()}\n","\n","# # Export results\n","# with open(f'/content/drive/MyDrive/Group_19/01.Dataset/Predictions/predictions_model3_{method}_{number_test_users}_users.json', 'w') as json_file:\n","#     json.dump(final_recommended_ids_multiple, json_file)"],"metadata":{"id":"dlWdu8kn7HJc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["PiQ2oebyxco3","sFRePjiQXuvB"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}